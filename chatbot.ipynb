{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_-zKhBiCTwS",
        "outputId": "45c2d57b-f7a7-4378-b289-3cef1223c8c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5vxSN98Czfk",
        "outputId": "93c59557-57f2-4e28-f157-71454114ba54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/train_qa.csv\")\n",
        "\n",
        "print(\"File loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH_Fa0L2C97a",
        "outputId": "00f26c23-8e8a-4fef-94ee-96ddef2adfaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Vocabulary Size: 36\n",
            "Max Story Length: 156\n",
            "Max Question Length: 6\n",
            "\n",
            "--- Final Data Shapes ---\n",
            "Training Stories shape: (8000, 156)\n",
            "Training Questions shape: (8000, 6)\n",
            "Training Answers shape: (8000,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "EMBEDDING_DIM = 64 \n",
        "\n",
        "\n",
        "stories = df['story'].values.tolist()\n",
        "questions = df['question'].values.tolist()\n",
        "\n",
        "answers = df['answer'].values.tolist()\n",
        "\n",
        "tokenizer = Tokenizer(filters=[], lower=True)\n",
        "tokenizer.fit_on_texts(stories + questions)\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(f\"Total Vocabulary Size: {VOCAB_SIZE}\")\n",
        "\n",
        "\n",
        "story_sequences = tokenizer.texts_to_sequences(stories)\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "\n",
        "\n",
        "MAX_STORY_LEN = max(len(s) for s in story_sequences)\n",
        "MAX_QUESTION_LEN = max(len(q) for q in question_sequences)\n",
        "\n",
        "print(f\"Max Story Length: {MAX_STORY_LEN}\")\n",
        "print(f\"Max Question Length: {MAX_QUESTION_LEN}\")\n",
        "\n",
        "\n",
        "padded_stories = pad_sequences(story_sequences, maxlen=MAX_STORY_LEN, padding='post')\n",
        "padded_questions = pad_sequences(question_sequences, maxlen=MAX_QUESTION_LEN, padding='post')\n",
        "\n",
        "\n",
        "target_answers = np.array([1 if a.lower() == 'yes' else 0 for a in answers])\n",
        "\n",
        "\n",
        "train_stories, test_stories, train_questions, test_questions, y_train, y_test = train_test_split(\n",
        "    padded_stories, padded_questions, target_answers, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Data Shapes ---\")\n",
        "print(f\"Training Stories shape: {train_stories.shape}\")\n",
        "print(f\"Training Questions shape: {train_questions.shape}\")\n",
        "print(f\"Training Answers shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lMdeMnSZDNGP",
        "outputId": "809f3db8-fbda-4fea-aefc-40a17d367d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras Dual-Encoder LSTM Model Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ story_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ question_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │ story_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ story_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │ question_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ question_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ story_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ question_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │      \u001b[38;5;34m2,304\u001b[0m │ story_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ story_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │      \u001b[38;5;34m2,304\u001b[0m │ question_input[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ question_input[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,977</span> (308.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,977\u001b[0m (308.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,977</span> (308.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,977\u001b[0m (308.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Model Training...\n",
            "Epoch 1/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.4959 - loss: 0.6936 - val_accuracy: 0.5120 - val_loss: 0.6921\n",
            "Epoch 2/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.5814 - loss: 0.6772 - val_accuracy: 0.6470 - val_loss: 0.6371\n",
            "Epoch 3/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6525 - loss: 0.6298 - val_accuracy: 0.6765 - val_loss: 0.6071\n",
            "Epoch 4/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6711 - loss: 0.6059 - val_accuracy: 0.7240 - val_loss: 0.5755\n",
            "Epoch 5/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6963 - loss: 0.5767 - val_accuracy: 0.7235 - val_loss: 0.5475\n",
            "Epoch 6/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7154 - loss: 0.5534 - val_accuracy: 0.7505 - val_loss: 0.5221\n",
            "Epoch 7/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7280 - loss: 0.5408 - val_accuracy: 0.7590 - val_loss: 0.5087\n",
            "Epoch 8/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7489 - loss: 0.5197 - val_accuracy: 0.7685 - val_loss: 0.4949\n",
            "Epoch 9/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7483 - loss: 0.5160 - val_accuracy: 0.7660 - val_loss: 0.4869\n",
            "Epoch 10/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7695 - loss: 0.4908 - val_accuracy: 0.7685 - val_loss: 0.4880\n",
            "Epoch 11/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7605 - loss: 0.4915 - val_accuracy: 0.7750 - val_loss: 0.4600\n",
            "Epoch 12/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7838 - loss: 0.4670 - val_accuracy: 0.7830 - val_loss: 0.4542\n",
            "Epoch 13/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7830 - loss: 0.4495 - val_accuracy: 0.7960 - val_loss: 0.4395\n",
            "Epoch 14/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7823 - loss: 0.4472 - val_accuracy: 0.8035 - val_loss: 0.4346\n",
            "Epoch 15/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7957 - loss: 0.4356 - val_accuracy: 0.8050 - val_loss: 0.4378\n",
            "Epoch 16/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7991 - loss: 0.4220 - val_accuracy: 0.8025 - val_loss: 0.4256\n",
            "Epoch 17/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8036 - loss: 0.4116 - val_accuracy: 0.8265 - val_loss: 0.4027\n",
            "Epoch 18/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8179 - loss: 0.3892 - val_accuracy: 0.8215 - val_loss: 0.4093\n",
            "Epoch 19/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8159 - loss: 0.3864 - val_accuracy: 0.8360 - val_loss: 0.3890\n",
            "Epoch 20/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8227 - loss: 0.3762 - val_accuracy: 0.8250 - val_loss: 0.3992\n",
            "\n",
            "--- Training Complete ---\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 36         \n",
        "MAX_STORY_LEN = 156     \n",
        "MAX_QUESTION_LEN = 6    \n",
        "EMBEDDING_DIM = 64      \n",
        "\n",
        "\n",
        "story_input = Input(shape=(MAX_STORY_LEN,), dtype='int32', name='story_input')\n",
        "\n",
        "\n",
        "story_embedding = Embedding(\n",
        "    input_dim=VOCAB_SIZE,\n",
        "    output_dim=EMBEDDING_DIM,\n",
        "    mask_zero=True \n",
        ")(story_input)\n",
        "\n",
        "\n",
        "story_lstm = LSTM(EMBEDDING_DIM)(story_embedding)\n",
        "\n",
        "\n",
        "\n",
        "question_input = Input(shape=(MAX_QUESTION_LEN,), dtype='int32', name='question_input')\n",
        "\n",
        "\n",
        "question_embedding = Embedding(\n",
        "    input_dim=VOCAB_SIZE,\n",
        "    output_dim=EMBEDDING_DIM,\n",
        "    mask_zero=True\n",
        ")(question_input)\n",
        "\n",
        "\n",
        "question_lstm = LSTM(EMBEDDING_DIM)(question_embedding)\n",
        "\n",
        "\n",
        "\n",
        "merged = concatenate([story_lstm, question_lstm])\n",
        "\n",
        "\n",
        "dense1 = Dense(EMBEDDING_DIM, activation='relu')(merged)\n",
        "dense1 = Dropout(0.5)(dense1) \n",
        "\n",
        "\n",
        "output_tensor = Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "\n",
        "model = Model(inputs=[story_input, question_input], outputs=output_tensor)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Keras Dual-Encoder LSTM Model Summary:\")\n",
        "model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(\"\\nStarting Model Training...\")\n",
        "\n",
        "history = model.fit(\n",
        "    x=[train_stories, train_questions], \n",
        "    y=y_train,\n",
        "    batch_size=64,\n",
        "    epochs=20, \n",
        "    validation_data=([test_stories, test_questions], y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THnGIq70DlH5",
        "outputId": "99e13a51-84ad-4954-c3c7-a2378bebcaf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Test 1 ---\n",
            "Story: Mary moved to the bathroom . Sandra journeyed to the bedroom . Daniel went back to the hallway . Mary went back to the bedroom .\n",
            "Question: Is Mary in the bedroom ?\n",
            "Prediction: Yes (Confidence: 0.8370)\n",
            "Expected Answer: Yes\n",
            "\n",
            "\n",
            "--- Test 2 ---\n",
            "Story: Daniel went back to the hallway . Sandra went to the kitchen . Daniel went back to the bathroom . Daniel picked up the football there . Daniel went to the bedroom .\n",
            "Question: Is Daniel in the hallway ?\n",
            "Prediction: No (Confidence: 0.0187)\n",
            "Expected Answer: No\n"
          ]
        }
      ],
      "source": [
        ".\n",
        "\n",
        "def predict_story_answer(story: str, question: str):\n",
        "    \"\"\"\n",
        "    Takes a raw story and question, preprocesses them, and returns a Yes/No answer.\n",
        "    \"\"\"\n",
        "   \n",
        "    story_seq = tokenizer.texts_to_sequences([story])\n",
        "    question_seq = tokenizer.texts_to_sequences([question])\n",
        "\n",
        "    \n",
        "    padded_story = pad_sequences(story_seq, maxlen=MAX_STORY_LEN, padding='post')\n",
        "    padded_question = pad_sequences(question_seq, maxlen=MAX_QUESTION_LEN, padding='post')\n",
        "\n",
        " \n",
        "    prediction = model.predict([padded_story, padded_question], verbose=0)\n",
        "\n",
        "    \n",
        "    probability = prediction[0][0]\n",
        "\n",
        "   \n",
        "    if probability > 0.5:\n",
        "        answer = \"Yes\"\n",
        "    else:\n",
        "        answer = \"No\"\n",
        "\n",
        "    return answer, probability\n",
        "\n",
        "\n",
        "test_story_1 = \"Mary moved to the bathroom . Sandra journeyed to the bedroom . Daniel went back to the hallway . Mary went back to the bedroom .\"\n",
        "test_question_1 = \"Is Mary in the bedroom ?\"\n",
        "\n",
        "\n",
        "test_story_2 = \"Daniel went back to the hallway . Sandra went to the kitchen . Daniel went back to the bathroom . Daniel picked up the football there . Daniel went to the bedroom .\"\n",
        "test_question_2 = \"Is Daniel in the hallway ?\"\n",
        "\n",
        "\n",
        "ans1, prob1 = predict_story_answer(test_story_1, test_question_1)\n",
        "ans2, prob2 = predict_story_answer(test_story_2, test_question_2)\n",
        "\n",
        "\n",
        "print(f\"\\n--- Test 1 ---\")\n",
        "print(f\"Story: {test_story_1}\")\n",
        "print(f\"Question: {test_question_1}\")\n",
        "print(f\"Prediction: {ans1} (Confidence: {prob1:.4f})\")\n",
        "print(f\"Expected Answer: Yes\\n\")\n",
        "\n",
        "print(f\"\\n--- Test 2 ---\")\n",
        "print(f\"Story: {test_story_2}\")\n",
        "print(f\"Question: {test_question_2}\")\n",
        "print(f\"Prediction: {ans2} (Confidence: {prob2:.4f})\")\n",
        "print(f\"Expected Answer: No\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxHbEbKfD772",
        "outputId": "522c6032-01b7-4680-e804-7bc821d1c1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 1000 test samples.\n",
            "\n",
            "Test data preprocessing complete.\n",
            "Test Stories shape: (1000, 156)\n",
            "Test Answers shape: (1000,)\n",
            "\n",
            "--- Running Final Model Evaluation on Test Set ---\n",
            "\n",
            "✅ Final Test Loss: 0.3972\n",
            "✅ Final Test Accuracy: 0.8160\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "TEST_CSV_FILE_PATH = '/content/drive/MyDrive/test_qa.csv'\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(TEST_CSV_FILE_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The test file '{TEST_CSV_FILE_PATH}' was not found. Please verify the path.\")\n",
        "    raise\n",
        "\n",
        "print(f\"Successfully loaded {len(df_test)} test samples.\")\n",
        "\n",
        "\n",
        "test_stories_raw = df_test['story'].values.tolist()\n",
        "test_questions_raw = df_test['question'].values.tolist()\n",
        "test_answers_raw = df_test['answer'].values.tolist()\n",
        "\n",
        "\n",
        "test_stories_seq = tokenizer.texts_to_sequences(test_stories_raw)\n",
        "test_questions_seq = tokenizer.texts_to_sequences(test_questions_raw)\n",
        "\n",
        "\n",
        "padded_test_stories = pad_sequences(test_stories_seq, maxlen=MAX_STORY_LEN, padding='post')\n",
        "padded_test_questions = pad_sequences(test_questions_seq, maxlen=MAX_QUESTION_LEN, padding='post')\n",
        "\n",
        "\n",
        "y_test_final = np.array([1 if a.lower() == 'yes' else 0 for a in test_answers_raw])\n",
        "\n",
        "print(\"\\nTest data preprocessing complete.\")\n",
        "print(f\"Test Stories shape: {padded_test_stories.shape}\")\n",
        "print(f\"Test Answers shape: {y_test_final.shape}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Running Final Model Evaluation on Test Set ---\")\n",
        "\n",
        "loss, accuracy = model.evaluate(\n",
        "    x=[padded_test_stories, padded_test_questions],\n",
        "    y=y_test_final,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Final Test Loss: {loss:.4f}\")\n",
        "print(f\"✅ Final Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3U7ze5pEVVF",
        "outputId": "1c61b27a-bbf8-44fd-c462-55336b9f4ee7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model saved successfully to: /content/drive/MyDrive/story_qa_lstm_model.h5\n",
            "✅ Tokenizer saved successfully to: /content/drive/MyDrive/story_qa_tokenizer.pickle\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "\n",
        "SAVE_DIR = '/content/drive/MyDrive'\n",
        "\n",
        "\n",
        "import os\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'story_qa_lstm_model.h5')\n",
        "TOKENIZER_PATH = os.path.join(SAVE_DIR, 'story_qa_tokenizer.pickle')\n",
        "\n",
        "save_model(model, MODEL_PATH)\n",
        "print(f\" Model saved successfully to: {MODEL_PATH}\")\n",
        "\n",
        "\n",
        "with open(TOKENIZER_PATH, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f\" Tokenizer saved successfully to: {TOKENIZER_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJegDsEGIaJZ",
        "outputId": "84d00338-5696-4c4e-8404-f98b90b4ae2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model successfully re-saved in the modern .keras format to: /content/drive/MyDrive/story_qa_best_lstm_model.keras\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "\n",
        "KERAS_MODEL_PATH = '/content/drive/MyDrive/story_qa_best_lstm_model.keras'\n",
        "\n",
        "\n",
        "save_model(model, KERAS_MODEL_PATH)\n",
        "\n",
        "print(f\"✅ Model successfully re-saved in the modern .keras format to: {KERAS_MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OPp-7upIw3_",
        "outputId": "f0f872ea-a913-42bd-bb4f-a56c172f805d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tokenizer loaded successfully.\n",
            "✅ Model loaded successfully (Native .keras format).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 14 variables whereas the saved optimizer has 26 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Test of Loaded Model ---\n",
            "Story: Mary moved to the garden. Sandra journeyed to the kitchen. Mary went back to the bedroom.\n",
            "Question: Is Mary in the kitchen?\n",
            "Prediction: No (Confidence: 0.2269)\n",
            "Expected Answer: No\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "\n",
        "\n",
        "KERAS_MODEL_PATH = '/content/drive/MyDrive/story_qa_best_lstm_model.keras'\n",
        "TOKENIZER_PATH = \"/content/drive/MyDrive/story_qa_tokenizer.pickle\"\n",
        "\n",
        "MAX_STORY_LEN = 156\n",
        "MAX_QUESTION_LEN = 6\n",
        "\n",
        "try:\n",
        "    with open(TOKENIZER_PATH, 'rb') as handle:\n",
        "        loaded_tokenizer = pickle.load(handle)\n",
        "    print(\"✅ Tokenizer loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Tokenizer file not found at {TOKENIZER_PATH}\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    loaded_model = load_model(KERAS_MODEL_PATH)\n",
        "    print(\"✅ Model loaded successfully (Native .keras format).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "\n",
        "def predict_loaded_answer(story: str, question: str, tokenizer_obj, model_obj):\n",
        "    story_seq = tokenizer_obj.texts_to_sequences([story])\n",
        "    question_seq = tokenizer_obj.texts_to_sequences([question])\n",
        "\n",
        "    padded_story = pad_sequences(story_seq, maxlen=MAX_STORY_LEN, padding='post')\n",
        "    padded_question = pad_sequences(question_seq, maxlen=MAX_QUESTION_LEN, padding='post')\n",
        "\n",
        "    \n",
        "    prediction = model_obj.predict([padded_story, padded_question], verbose=0)\n",
        "    probability = prediction[0][0]\n",
        "    answer = \"Yes\" if probability > 0.5 else \"No\"\n",
        "\n",
        "    return answer, probability\n",
        "\n",
        "test_story = \"Mary moved to the garden. Sandra journeyed to the kitchen. Mary went back to the bedroom.\"\n",
        "test_question = \"Is Mary in the kitchen?\"\n",
        "\n",
        "\n",
        "ans, prob = predict_loaded_answer(\n",
        "    test_story,\n",
        "    test_question,\n",
        "    loaded_tokenizer,\n",
        "    loaded_model\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Test of Loaded Model ---\")\n",
        "print(f\"Story: {test_story}\")\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Prediction: {ans} (Confidence: {prob:.4f})\")\n",
        "print(f\"Expected Answer: No\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
